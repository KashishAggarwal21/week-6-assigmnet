# week-6-assigmnet


# ğŸ“˜ ETL Project using Azure Data Factory â€“ Week 6

This project demonstrates how to build an **end-to-end ETL workflow using Azure Data Factory (ADF)** that connects to:

- A **local SQL Server (on-premises)**
- An **external SFTP server**

The extracted data is loaded into:

- **Azure SQL Database**
- **Azure Data Lake Storage (ADLS Gen2)**

The solution includes **incremental loading**, and a **custom trigger** scheduled to run automatically on the **last Saturday of every month**.

## ğŸ“‚ Data Sources

We used the **NYC Restaurant Inspection** dataset, which is updated monthly â€” making it ideal for testing incremental load logic and periodic automation.

## ğŸ§± Architecture Overview

<p align="center">
  <img src="./images/diagram.png" alt="Architecture Diagram" />
</p>

## ğŸ”„ ETL Pipelines Built

### ğŸŸ¢ Pipeline 1: From On-Prem SQL Server to Azure SQL DB
- Transfers records from a local SQL table to an Azure-hosted SQL Database.

### ğŸ”µ Pipeline 2: From SFTP Server to ADLS
- Fetches CSV files from an SFTP location and stores them in ADLS Gen2.

### ğŸŸ£ Master Pipeline
- Chains both pipelines and includes a trigger to automate execution on the **last Saturday of every month**.

## ğŸ› ï¸ Source Configurations

### Local SQL Server:
- Setup through **SQL Server Management Studio (SSMS)**
- Hosted the NYC restaurant data locally.

### SFTP:
- Configured access using credentials via tools like **WinSCP**
- Contained multiple CSVs partitioned by year.

## ğŸ¯ Targets

### Azure SQL Database:
- Hosted in Azure within the resource group `week6`
- Used to store cleaned restaurant inspection records.

### Azure Data Lake (ADLS Gen2):
- Organized into folders:
  - `SFTP-DATA`
  - `Restaurant_Inspection`

## ğŸ”§ Pipeline 1 Details: Local SQL Server â Azure SQL DB

### âœ… Linked Services
- **Local SQL Server** via SHIR
- **Azure SQL Database**

### ğŸ“¦ Datasets
- Source: `sqlservertable1`
- Destination: `azuresqltable1`

### ğŸ” Incremental Logic with Watermark

1. **Lookup latest inspection date** from watermark table
2. **Filter** new rows using this date
3. **Update watermark** post successful load

```sql
SELECT MAX([INSPECTION DATE]) FROM dbo.NYC_Restaurant
```

## ğŸŒ Pipeline 2: SFTP â Azure Data Lake

### âœ… Linked Services
- **SFTP server**
- **Azure Data Lake**

### ğŸ“¦ Datasets
- Source: CSV files from SFTP
- Destination: ADLS folders

ğŸ’¡ *This can be extended using watermark logic for incremental SFTP processing.*

## ğŸ” Master Pipeline & Trigger

Combines both pipelines and adds a **recurring trigger**:
- Runs at **7:00 AM** on the **last Saturday of every month**

### ğŸ“… Trigger Setup:
- Type: Monthly
- Day: Last Saturday
- Time: 07:00 AM

## âœ… Project Status

| Component                    | Status     |
|-----------------------------|------------|
| SHIR setup for on-prem data | âœ… Complete |
| SFTP to ADLS pipeline        | âœ… Complete |
| Incremental loading logic    | âœ… Complete |
| Trigger configuration        | âœ… Complete |

This project helped me implement a real-world ETL architecture using Azure services. It covers **multiple data sources**, **automated scheduling**, and **optimized incremental loading** â€” essential for any modern data pipeline.
